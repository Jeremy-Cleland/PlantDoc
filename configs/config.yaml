# General project info
project_name: CBAM Classification
version: 1.0.0

# Global settings
seed: 42  # Global seed for reproducibility

logging:
  level: INFO
  use_colors: true
  log_to_file: true
  log_file: "command.log"

# Path configuration with improved structure
paths:
  # Base directories with clear organization
  output_dir: "outputs"
  models_dir: "outputs/models"
  logs_dir: "outputs/logs"
  viz_dir: "outputs/visualizations"
  results_dir: "outputs/results"
  cache_dir: "outputs/cache"

  experiment_name: "cbam_only_resnet18_v2"
  
  # Data directories - relative to project root
  raw_dir: "data/raw"
  preprocessed_dir: "data/preprocessed"

# Data configuration
data:
  dataset_name: PlantDisease
  train_val_test_split: [0.7, 0.15, 0.15]
  random_seed: 42
  class_names:
    - Apple_scab
    - Apple_black_rot
    - Apple_cedar_apple_rust
    - Apple_healthy
    - Background_without_leaves
    - Blueberry_healthy
    - Cherry_powdery_mildew
    - Cherry_healthy
    - Corn_gray_leaf_spot
    - Corn_common_rust
    - Corn_northern_leaf_blight
    - Corn_healthy
    - Grape_black_rot
    - Grape_black_measles
    - Grape_leaf_blight
    - Grape_healthy
    - Orange_haunglongbing
    - Peach_bacterial_spot
    - Peach_healthy
    - Pepper_bacterial_spot
    - Pepper_healthy
    - Potato_early_blight
    - Potato_healthy
    - Potato_late_blight
    - Raspberry_healthy
    - Soybean_healthy
    - Squash_powdery_mildew
    - Strawberry_healthy
    - Strawberry_leaf_scorch
    - Tomato_bacterial_spot
    - Tomato_early_blight
    - Tomato_healthy
    - Tomato_late_blight
    - Tomato_leaf_mold
    - Tomato_septoria_leaf_spot
    - Tomato_spider_mites_two-spotted_spider_mite
    - Tomato_target_spot
    - Tomato_mosaic_virus
    - Tomato_yellow_leaf_curl_virus

# DataLoader configuration
loader:
  batch_size: 128  # Reduced from 256 for better stability with increased augmentations
  num_workers: 6
  pin_memory: true
  drop_last: false
  prefetch_factor: 2
  persistent_workers: true

# Preprocessing configuration for data transformations
preprocessing:
  # Common settings for all splits
  resize: [256, 256]
  center_crop: [224, 224]
  normalize:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  
  # Legacy format (keeping for compatibility)
  train:
    resize_height: 256
    resize_width: 256
    horizontal_flip_prob: 0.5
    vertical_flip_prob: 0.0
    random_crop_height: 224
    random_crop_width: 224
    random_crop_prob: 0.5
    rotation_prob: 0.5
    rotation_limit: 45
    brightness_contrast_prob: 0.5
    brightness_limit: 0.2
    contrast_limit: 0.2
    normalize: true
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  
  val:
    resize_height: 256
    resize_width: 256
    center_crop_height: 224
    center_crop_width: 224
    normalize: true
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  
  test:
    resize_height: 256
    resize_width: 256
    center_crop_height: 224
    center_crop_width: 224
    normalize: true
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

# Augmentation settings - Enhanced strategy
augmentation:
  train:
    # Basic augmentations - disabled when using RandAugment
    horizontal_flip: false
    vertical_flip: false
    random_rotate: 45
    random_resized_crop:
      size: [224, 224]
      scale: [0.8, 1.0]
      ratio: [0.75, 1.33]
    color_jitter:
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
    random_brightness_contrast:
      brightness_limit: 0.2
      contrast_limit: 0.2
      p: 0.5
    shift_scale_rotate:
      shift_limit: 0.05
      scale_limit: 0.05
      rotate_limit: 15
      p: 0.5
    cutout:
      enabled: false  # Disabled in favor of CutMix
      num_holes: 4
      max_h_size: 16
      max_w_size: 16
      p: 0.5
      
    # Advanced augmentation strategies (now using multiple strategies)
    rand_augment:
      enabled: true
      num_ops: 3      # Increased from 2 to 3 for more diverse augmentations
      magnitude: 9    # Increased from 7 to 9 for stronger augmentations
      p: 0.95         # Increased from 0.9 to 0.95 for more consistent application
      
    augmix:
      enabled: false  # Still disabled to avoid conflicts
      severity: 3
      width: 3
      depth: -1
      alpha: 1.0
      p: 0.5
      
    # Batch-level augmentations
    mixup:
      enabled: true   # Enabled MixUp alongside CutMix 
      alpha: 0.4      # Modest alpha value to balance with CutMix
      p: 0.3          # Lower probability than CutMix to avoid overaugmentation
      
    cutmix:
      enabled: true
      alpha: 1.0
      p: 0.5          # Increased from 0.3 to 0.5 for better regularization

# Model configuration
model:
  # Base model settings
  name: cbam_only_resnet18
  num_classes: 39
  pretrained: true
  in_channels: 3
  input_size: [224, 224]
  
  # Head settings
  head_type: "residual"  # Keep residual head which performs well
  hidden_dim: 256
  dropout_rate: 0.15     # Adjusted from 0.09 to 0.15 for better regularization
  use_residual_head: true
  
  # Backbone settings
  freeze_backbone: true  # Will be gradually unfrozen via transfer_learning settings
  feature_fusion: true   # Keep feature fusion enabled
  
  # CBAM settings - Enhanced attention
  reduction_ratio: 8     # Reduced from 16 to 8 for stronger attention
  spatial_kernel_size: 5 # Changed from 7 to 5 for finer spatial attention
  
  # Regularization within model components
  regularization:
    stochastic_depth_prob: 0.1   # Increased from 0.04 for better regularization
    drop_path_prob: 0.25         # Increased from 0.19 for better regularization

# Optimizer settings - Tuned for better convergence
optimizer:
  name: adamw
  lr: 0.0005                     # Reduced from 0.001 for more stability
  weight_decay: 5e-5             # Increased from 2.2e-5 for better regularization
  betas: [0.9, 0.999]
  eps: 1e-8
  momentum: 0.9                  # For SGD if used
  nesterov: false
  differential_lr: true          # Keep differential learning rate
  differential_lr_factor: 0.1

# Loss function settings - Enhanced weighting
loss:
  name: combined
  components:
    - name: weighted_cross_entropy
      weight: 0.7                # Decreased from 1.0
      reduction: mean
      label_smoothing: 0.15      # Increased from 0.1
    - name: focal
      weight: 0.7                # Increased from 0.5
      gamma: 2.5                 # Increased from 2.0
      alpha: 0.25
      reduction: mean
      label_smoothing: 0.0

# Learning rate scheduler - Changed to cosine annealing with warm restarts
scheduler:
  name: cosine_annealing_warm_restarts  # Changed from reduce_on_plateau
  T_0: 10                               # Initial restart period
  T_mult: 2                             # Multiplier for restart periods
  eta_min: 1e-6                         # Minimum learning rate
  step_mode: epoch                      # epoch or step, controls when scheduler.step() is called
  warmup_epochs: 5
  warmup_start_lr: 1.0e-5
  log_changes: true
  enabled: true

# Training configuration - Extended training time
training:
  epochs: 150                    # Increased from 100 to allow longer training
  batch_size: 128                # Reduced from 256 to accommodate stronger augmentations
  learning_rate: 0.0005          # Match optimizer lr
  weight_decay: 5e-5             # Match optimizer weight_decay
  deterministic: false
  use_mixed_precision: true
  gradient_clip_val: 1.0
  precision: "float16"
  dropout: 0.15                  # Match model dropout_rate

# Transfer learning settings - More gradual unfreezing
transfer_learning:
  initial_frozen_epochs: 10      # Increased from 5 to 10 for better initialization
  finetune_lr_factor: 0.05       # Decreased from 0.1 to 0.05 for finer tuning

# Callbacks configuration - Enhanced callbacks
callbacks:
  early_stopping:
    enabled: true
    monitor: val_loss
    patience: 15                # Increased from 10 due to longer training and cosine schedule
    mode: min
    min_delta: 0.001
    restore_best_weights: true
    verbose: true
  model_checkpoint:
    enabled: true
    dirpath: "outputs/models/checkpoints"
    filename: "epoch_{epoch:03d}_{val_loss:.4f}"
    monitor: val_loss
    save_best_only: true
    best_filename: "best_model.pth"
    mode: min
    save_last: true
    last_filename: "last_model.pth"
    save_optimizer: true
    save_freq: "epoch"
    verbose: true
    max_save: 5                 # Increased from 3 to keep more checkpoints
  learning_rate_monitor:
    enabled: true
    logging_interval: epoch
    verbose: true
  metrics_logger:
    enabled: true
  tensorboard:
    enabled: true
    log_dir: "outputs/logs/tensorboard"
    log_graph: false
    histogram_freq: 1
  gradcam:
    enabled: true
    frequency: 10
    n_samples: 10
    input_size: [224, 224]
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    output_dir: "outputs/visualizations/gradcam"
    # Class names remain the same
    class_names:
      - Apple_scab
      - Apple_black_rot
      - Apple_cedar_apple_rust
      - Apple_healthy
      - Background_without_leaves
      - Blueberry_healthy
      - Cherry_powdery_mildew
      - Cherry_healthy
      - Corn_gray_leaf_spot
      - Corn_common_rust
      - Corn_northern_leaf_blight
      - Corn_healthy
      - Grape_black_rot
      - Grape_black_measles
      - Grape_leaf_blight
      - Grape_healthy
      - Orange_haunglongbing
      - Peach_bacterial_spot
      - Peach_healthy
      - Pepper_bacterial_spot
      - Pepper_healthy
      - Potato_early_blight
      - Potato_healthy
      - Potato_late_blight
      - Raspberry_healthy
      - Soybean_healthy
      - Squash_powdery_mildew
      - Strawberry_healthy
      - Strawberry_leaf_scorch
      - Tomato_bacterial_spot
      - Tomato_early_blight
      - Tomato_healthy
      - Tomato_late_blight
      - Tomato_leaf_mold
      - Tomato_septoria_leaf_spot
      - Tomato_spider_mites_two-spotted_spider_mite
      - Tomato_target_spot
      - Tomato_mosaic_virus
      - Tomato_yellow_leaf_curl_virus
  attention_viz:
    enabled: true
    n_samples: 15               # Increased from 10 to provide more visualization examples
    
  # SHAP analysis
  shap:
    enabled: true               # Enabled from false for better interpretability
    num_samples: 20             # Increased from 10
    compare_with_gradcam: true  # Changed to true
    num_background_samples: 20  # Increased from 10
    output_subdir: "shap_analysis"
    dataset_split: "test"
    
  # Confidence monitoring
  confidence_monitor:
    enabled: true
    monitor_frequency: 1
    threshold_warning: 0.7
    ece_bins: 15               # Increased from 10 for finer-grained ECE measurement
    log_per_class: true

  # Stochastic Weight Averaging - Optimized
  swa:
    enabled: true
    swa_start_frac: 0.5        # Start earlier (50% through training) to allow more averaging time
    swa_lr: 0.0002             # Adjusted for new base learning rate
    anneal_epochs: 10          # Increased from 5 for smoother transitions
    anneal_strategy: "cos"
    update_bn_epochs: 5
  
  # Adaptive weight adjustment - Enhanced for imbalanced classes
  adaptive_weight:
    enabled: true
    focal_loss_gamma_range: [2.0, 5.0]  # Wider range to help with problematic classes
    center_loss_weight_range: [0.5, 2.0]  # Wider range to allow stronger adjustments
    adjust_frequency: 5                   # More frequent adjustments (every 5 epochs)

# Evaluation configuration
evaluation:
  split: test
  output_dir: "outputs/results/evaluation"
  metrics:
    accuracy: true
    precision: true
    recall: true
    f1: true
    confusion_matrix: true
  interpretability:
    gradcam: true
    num_samples: 20            # Increased from 10
    target_layers: ["layer4"]
    output_dir: "outputs/visualizations/interpretability"
    # SHAP configuration - Enhanced
    shap:
      enabled: true            # Changed from false
      num_samples: 50
      compare_with_gradcam: true
      num_background_samples: 50
      output_subdir: "shap_analysis"
      dataset_split: "test"

# Hardware configuration
hardware:
  precision: "float16"          # Keep for MPS performance
  device: "mps"                 # Keep Apple Silicon GPU settings
  mps:                          # Apple Silicon GPU specific settings
    memory:
      monitor: true
      clear_cache_freq: 5       # More frequent cache clearing (from 10 to 5)
      deep_clean_after_val: true
      enable_sync_for_timing: true
      limit_fraction: 0.85      # Reduced from 0.95 to 0.85 for safer memory usage
    model:
      use_channels_last: true
    profiling:
      enabled: true             # Changed from false to enable profiling
      output_dir: "outputs/profiling"
    reproducibility:
      set_mps_seed: true
  cuda:
    benchmark: true
    deterministic: false
    cudnn_deterministic: false

# Class-specific weighting for problematic classes
class_weights:
  enabled: true
  method: "inverse_frequency"  # Options: manual, inverse_frequency, effective_samples
  manual_weights:
    # Special focus on Corn classes that showed lower performance
    8: 1.5    # Corn_gray_leaf_spot
    10: 1.3   # Corn_northern_leaf_blight
  smooth_factor: 0.2